7767517
54 58
Input            data             0 1 data 0=3 1=227 2=227
BatchNorm        data_bn          1 1 data data_bn 0=3
Scale            data_scale       1 1 data_bn data_bn_data_scale 0=3 1=1
Convolution      conv1            1 1 data_bn_data_scale conv1 0=64 1=7 2=1 3=2 4=3 5=1 6=9408
BatchNorm        conv1_bn         1 1 conv1 conv1_conv1_bn 0=64
Scale            conv1_scale      1 1 conv1_conv1_bn conv1_conv1_scale 0=64 1=1
ReLU             conv1_relu       1 1 conv1_conv1_scale conv1_conv1_relu 0=0.000000
Pooling          conv1_pool       1 1 conv1_conv1_relu conv1_pool 0=0 1=3 2=2 3=0 4=0
Split            splitncnn_0      1 2 conv1_pool conv1_pool_splitncnn_0 conv1_pool_splitncnn_1
Convolution      layer_64_1_conv1 1 1 conv1_pool_splitncnn_1 layer_64_1_conv1 0=64 1=3 2=1 3=1 4=1 5=0 6=36864
BatchNorm        layer_64_1_bn2   1 1 layer_64_1_conv1 layer_64_1_conv1_layer_64_1_bn2 0=64
Scale            layer_64_1_scale2 1 1 layer_64_1_conv1_layer_64_1_bn2 layer_64_1_conv1_layer_64_1_scale2 0=64 1=1
ReLU             layer_64_1_relu2 1 1 layer_64_1_conv1_layer_64_1_scale2 layer_64_1_conv1_layer_64_1_relu2 0=0.000000
Convolution      layer_64_1_conv2 1 1 layer_64_1_conv1_layer_64_1_relu2 layer_64_1_conv2 0=64 1=3 2=1 3=1 4=1 5=0 6=36864
Eltwise          layer_64_1_sum   2 1 layer_64_1_conv2 conv1_pool_splitncnn_0 layer_64_1_sum 0=1 -23301=0
BatchNorm        layer_128_1_bn1  1 1 layer_64_1_sum layer_128_1_bn1 0=64
Scale            layer_128_1_scale1 1 1 layer_128_1_bn1 layer_128_1_bn1_layer_128_1_scale1 0=64 1=1
ReLU             layer_128_1_relu1 1 1 layer_128_1_bn1_layer_128_1_scale1 layer_128_1_bn1_layer_128_1_relu1 0=0.000000
Split            splitncnn_1      1 2 layer_128_1_bn1_layer_128_1_relu1 layer_128_1_bn1_layer_128_1_relu1_splitncnn_0 layer_128_1_bn1_layer_128_1_relu1_splitncnn_1
Convolution      layer_128_1_conv1 1 1 layer_128_1_bn1_layer_128_1_relu1_splitncnn_1 layer_128_1_conv1 0=128 1=3 2=1 3=2 4=1 5=0 6=73728
BatchNorm        layer_128_1_bn2  1 1 layer_128_1_conv1 layer_128_1_conv1_layer_128_1_bn2 0=128
Scale            layer_128_1_scale2 1 1 layer_128_1_conv1_layer_128_1_bn2 layer_128_1_conv1_layer_128_1_scale2 0=128 1=1
ReLU             layer_128_1_relu2 1 1 layer_128_1_conv1_layer_128_1_scale2 layer_128_1_conv1_layer_128_1_relu2 0=0.000000
Convolution      layer_128_1_conv2 1 1 layer_128_1_conv1_layer_128_1_relu2 layer_128_1_conv2 0=128 1=3 2=1 3=1 4=1 5=0 6=147456
Convolution      layer_128_1_conv_expand 1 1 layer_128_1_bn1_layer_128_1_relu1_splitncnn_0 layer_128_1_conv_expand 0=128 1=1 2=1 3=2 4=0 5=0 6=8192
Eltwise          layer_128_1_sum  2 1 layer_128_1_conv2 layer_128_1_conv_expand layer_128_1_sum 0=1 -23301=0
BatchNorm        layer_256_1_bn1  1 1 layer_128_1_sum layer_256_1_bn1 0=128
Scale            layer_256_1_scale1 1 1 layer_256_1_bn1 layer_256_1_bn1_layer_256_1_scale1 0=128 1=1
ReLU             layer_256_1_relu1 1 1 layer_256_1_bn1_layer_256_1_scale1 layer_256_1_bn1_layer_256_1_relu1 0=0.000000
Split            splitncnn_2      1 2 layer_256_1_bn1_layer_256_1_relu1 layer_256_1_bn1_layer_256_1_relu1_splitncnn_0 layer_256_1_bn1_layer_256_1_relu1_splitncnn_1
Convolution      layer_256_1_conv1 1 1 layer_256_1_bn1_layer_256_1_relu1_splitncnn_1 layer_256_1_conv1 0=256 1=3 2=1 3=2 4=1 5=0 6=294912
BatchNorm        layer_256_1_bn2  1 1 layer_256_1_conv1 layer_256_1_conv1_layer_256_1_bn2 0=256
Scale            layer_256_1_scale2 1 1 layer_256_1_conv1_layer_256_1_bn2 layer_256_1_conv1_layer_256_1_scale2 0=256 1=1
ReLU             layer_256_1_relu2 1 1 layer_256_1_conv1_layer_256_1_scale2 layer_256_1_conv1_layer_256_1_relu2 0=0.000000
Convolution      layer_256_1_conv2 1 1 layer_256_1_conv1_layer_256_1_relu2 layer_256_1_conv2 0=256 1=3 2=1 3=1 4=1 5=0 6=589824
Convolution      layer_256_1_conv_expand 1 1 layer_256_1_bn1_layer_256_1_relu1_splitncnn_0 layer_256_1_conv_expand 0=256 1=1 2=1 3=2 4=0 5=0 6=32768
Eltwise          layer_256_1_sum  2 1 layer_256_1_conv2 layer_256_1_conv_expand layer_256_1_sum 0=1 -23301=0
BatchNorm        layer_512_1_bn1  1 1 layer_256_1_sum layer_512_1_bn1 0=256
Scale            layer_512_1_scale1 1 1 layer_512_1_bn1 layer_512_1_bn1_layer_512_1_scale1 0=256 1=1
ReLU             layer_512_1_relu1 1 1 layer_512_1_bn1_layer_512_1_scale1 layer_512_1_bn1_layer_512_1_relu1 0=0.000000
Split            splitncnn_3      1 2 layer_512_1_bn1_layer_512_1_relu1 layer_512_1_bn1_layer_512_1_relu1_splitncnn_0 layer_512_1_bn1_layer_512_1_relu1_splitncnn_1
Convolution      layer_512_1_conv1 1 1 layer_512_1_bn1_layer_512_1_relu1_splitncnn_1 layer_512_1_conv1 0=512 1=3 2=1 3=2 4=1 5=0 6=1179648
BatchNorm        layer_512_1_bn2  1 1 layer_512_1_conv1 layer_512_1_conv1_layer_512_1_bn2 0=512
Scale            layer_512_1_scale2 1 1 layer_512_1_conv1_layer_512_1_bn2 layer_512_1_conv1_layer_512_1_scale2 0=512 1=1
ReLU             layer_512_1_relu2 1 1 layer_512_1_conv1_layer_512_1_scale2 layer_512_1_conv1_layer_512_1_relu2 0=0.000000
Convolution      layer_512_1_conv2 1 1 layer_512_1_conv1_layer_512_1_relu2 layer_512_1_conv2 0=512 1=3 2=1 3=1 4=1 5=0 6=2359296
Convolution      layer_512_1_conv_expand 1 1 layer_512_1_bn1_layer_512_1_relu1_splitncnn_0 layer_512_1_conv_expand 0=512 1=1 2=1 3=2 4=0 5=0 6=131072
Eltwise          layer_512_1_sum  2 1 layer_512_1_conv2 layer_512_1_conv_expand layer_512_1_sum 0=1 -23301=0
BatchNorm        last_bn          1 1 layer_512_1_sum layer_512_1_sum_last_bn 0=512
Scale            last_scale       1 1 layer_512_1_sum_last_bn layer_512_1_sum_last_scale 0=512 1=1
ReLU             last_relu        1 1 layer_512_1_sum_last_scale layer_512_1_sum_last_relu 0=0.000000
Pooling          global_pool      1 1 layer_512_1_sum_last_relu global_pool 0=1 1=0 2=1 3=0 4=1
InnerProduct     fc8              1 1 global_pool fc8 0=2 1=1 2=1024
Softmax          prob             1 1 fc8 prob 0=0
